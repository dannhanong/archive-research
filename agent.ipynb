{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from constants import embed_model\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"index/\")\n",
    "index = load_index_from_storage(storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from constants import llm_model\n",
    "\n",
    "query_engine = index.as_query_engine(llm_model=llm_model, similarity_top_k=5)\n",
    "rag_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine, \n",
    "    name=\"research_paper_query_engine_tool\",\n",
    "    description=\"A RAG engine tool for querying research papers\"            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for key, prompt in prompts_dict.items():\n",
    "        display(Markdown(f'**Prompt:** {key}'))\n",
    "        print(prompt.get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt:** response_synthesizer:text_qa_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt:** response_synthesizer:refine_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import download_pdf, fetch_arxiv_papers\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from tools import download_pdf\n",
    "\n",
    "download_pdf_tool = FunctionTool.from_defaults(\n",
    "    download_pdf,\n",
    "    name=\"download_pdf_tool\",\n",
    "    description=\"A tool for downloading PDFs from URLs\"\n",
    ")\n",
    "\n",
    "fetch_arxiv_tool = FunctionTool.from_defaults(\n",
    "    fetch_arxiv_papers,\n",
    "    name=\"fetch_arxiv_tool\",\n",
    "    description=\"A tool for fetching papers from arXiv. Download the {max_results} most recent papers from arXiv.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    [download_pdf_tool,\n",
    "    rag_tool,\n",
    "    fetch_arxiv_tool],\n",
    "    llm=llm_model,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"\n",
    "I am interested in the topic of {topic}. Find papers in your knowledge database related to this topic.\n",
    "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, sumary, authors and link to download for papers related to {topic}'.'\n",
    "If there are not, could you fetch some papers from arXiv on this topic?\n",
    "\n",
    "IMPORTANT: do not download papers unless the user asks for it explicitly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 2540faf8-a638-47c9-b975-45d2efb9d0ef. Step input: \n",
      "I am interested in the topic of Multi-Model Models. Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, sumary, authors and link to download for papers related to Multi-Model Models'.'\n",
      "If there are not, could you fetch some papers from arXiv on this topic?\n",
      "\n",
      "IMPORTANT: do not download papers unless the user asks for it explicitly.\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The user is interested in finding papers related to Multi-Model Models. I can use the research_paper_query_engine_tool to search for relevant papers.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Multi-Model Models'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: Continuous multilinguality with language vectors  \n",
      "Authors: Robert Östling, Jörg Tiedemann  \n",
      "Summary: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.  \n",
      "PDF url: http://arxiv.org/pdf/1612.07486v2\n",
      "\n",
      "Title: The Geometry of Multilingual Language Model Representations  \n",
      "Authors: Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen  \n",
      "Summary: We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning.  \n",
      "PDF url: http://arxiv.org/pdf/2205.10964v2\n",
      "\n",
      "Title: Goldfish: Monolingual Language Models for 350 Languages  \n",
      "Authors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen  \n",
      "Summary: For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than bigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM 7.1B). To facilitate research that focuses on low-resource languages, we pre-train and release Goldfish, a suite of monolingual autoregressive Transformer language models up to 125M parameters for 350 languages. The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10x smaller. However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation. We release models trained on 5MB (350 languages), 10MB (288 languages), 100MB (166 languages), and 1GB (83 languages) of text data where available. The Goldfish models are available as baselines, fine-tuning sources, or augmentations to existing models in low-resource NLP research, and they are further useful for crosslinguistic studies requiring maximally comparable models across languages.  \n",
      "PDF url: http://arxiv.org/pdf/2408.10441v1\n",
      "\u001b[0m> Running step 819d0542-dd3d-435e-b9af-5f3f95392bb3. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have found relevant papers related to Multi-Model Models in the database.\n",
      "Answer: 1. Title: Continuous multilinguality with language vectors  \n",
      "   Authors: Robert Östling, Jörg Tiedemann  \n",
      "   Summary: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.  \n",
      "   PDF url: [Download PDF](http://arxiv.org/pdf/1612.07486v2)\n",
      "\n",
      "2. Title: The Geometry of Multilingual Language Model Representations  \n",
      "   Authors: Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen  \n",
      "   Summary: We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning.  \n",
      "   PDF url: [Download PDF](http://arxiv.org/pdf/2205.10964v2)\n",
      "\n",
      "3. Title: Goldfish: Monolingual Language Models for 350 Languages  \n",
      "   Authors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen  \n",
      "   Summary: For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than bigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM 7.1B). To facilitate research that focuses on low-resource languages, we pre-train and release Goldfish, a suite of monolingual autoregressive Transformer language models up to 125M parameters for 350 languages. The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10x smaller. However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation. We release models trained on 5MB (350 languages), 10MB (288 languages), 100MB (166 languages), and 1GB (83 languages) of text data where available. The Goldfish models are available as baselines, fine-tuning sources, or augmentations to existing models in low-resource NLP research, and they are further useful for crosslinguistic studies requiring maximally comparable models across languages.  \n",
      "   PDF url: [Download PDF](http://arxiv.org/pdf/2408.10441v1)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"Multi-Model Models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Title: Continuous multilinguality with language vectors  \n",
       "   Authors: Robert Östling, Jörg Tiedemann  \n",
       "   Summary: Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.  \n",
       "   PDF url: [Download PDF](http://arxiv.org/pdf/1612.07486v2)\n",
       "\n",
       "2. Title: The Geometry of Multilingual Language Model Representations  \n",
       "   Authors: Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen  \n",
       "   Summary: We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning.  \n",
       "   PDF url: [Download PDF](http://arxiv.org/pdf/2205.10964v2)\n",
       "\n",
       "3. Title: Goldfish: Monolingual Language Models for 350 Languages  \n",
       "   Authors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen  \n",
       "   Summary: For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than bigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM 7.1B). To facilitate research that focuses on low-resource languages, we pre-train and release Goldfish, a suite of monolingual autoregressive Transformer language models up to 125M parameters for 350 languages. The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10x smaller. However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation. We release models trained on 5MB (350 languages), 10MB (288 languages), 100MB (166 languages), and 1GB (83 languages) of text data where available. The Goldfish models are available as baselines, fine-tuning sources, or augmentations to existing models in low-resource NLP research, and they are further useful for crosslinguistic studies requiring maximally comparable models across languages.  \n",
       "   PDF url: [Download PDF](http://arxiv.org/pdf/2408.10441v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step a12d2e48-d064-423d-926f-871c8d07f9cf. Step input: Download the following papers:\n",
      "For each paper:\n",
      "1. Process one paper at a time\n",
      "2. State which paper number you are processing out of the total\n",
      "3. Save each file with a descriptive filename and .pdf extension\n",
      "4. Verify the download was successful before moving to the next paper\n",
      "5. Explicitly state when you are moving to the next paper\n",
      "6. Provide a final summary after all papers are downloaded\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The user has requested to download the papers related to Multi-Model Models. I will use the download_pdf_tool to download the papers one by one.\n",
      "Action: download_pdf_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/1612.07486v2', 'output_file_name': 'Continuous_multilinguality_with_language_vectors.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded to papers\\Continuous_multilinguality_with_language_vectors.pdf\n",
      "\u001b[0m> Running step 3d758880-2000-4ee9-9227-0d30e4429309. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The first paper has been successfully downloaded. Moving on to the next paper.\n",
      "Action: download_pdf_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2205.10964v2', 'output_file_name': 'The_Geometry_of_Multilingual_Language_Model_Representations.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded to papers\\The_Geometry_of_Multilingual_Language_Model_Representations.pdf\n",
      "\u001b[0m> Running step 8f0915ba-ca85-4cd4-af0a-58b8e2096880. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The second paper has been successfully downloaded. Moving on to the final paper.\n",
      "Action: download_pdf_tool\n",
      "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2408.10441v1', 'output_file_name': 'Goldfish_Monolingual_Language_Models_for_350_Languages.pdf'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded to papers\\Goldfish_Monolingual_Language_Models_for_350_Languages.pdf\n",
      "\u001b[0m> Running step 6411c7ad-1dd3-45ff-af1c-29b88282b1fd. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have successfully downloaded all the papers related to Multi-Model Models. Now, I will provide a final summary of the downloaded papers.\n",
      "Answer: 1. Continuous multilinguality with language vectors: This paper explores the use of continuous vector representations of language to improve inference about language varieties not seen during training. It empirically explores the capacity of multilingual language models and captures genetic relationships between languages.\n",
      "2. The Geometry of Multilingual Language Model Representations: The paper assesses how multilingual language models maintain a shared multilingual representation space while encoding language-sensitive information. It identifies stable language-sensitive and language-neutral axes in the models.\n",
      "3. Goldfish: Monolingual Language Models for 350 Languages: This paper introduces monolingual autoregressive Transformer language models for 350 languages to address the limitations of large multilingual models for low-resource languages. The models outperform larger multilingual models on certain metrics for low-resource languages.\n",
      "\n",
      "These papers provide valuable insights into the development and application of multi-model models in natural language processing.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(\"\"\"Download the following papers:\n",
    "For each paper:\n",
    "1. Process one paper at a time\n",
    "2. State which paper number you are processing out of the total\n",
    "3. Save each file with a descriptive filename and .pdf extension\n",
    "4. Verify the download was successful before moving to the next paper\n",
    "5. Explicitly state when you are moving to the next paper\n",
    "6. Provide a final summary after all papers are downloaded\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 0a3a8194-bf25-489a-817b-6418ec7d2a62. Step input: \n",
      "I am interested in the topic of Quantum Computing. Find papers in your knowledge database related to this topic.\n",
      "Use the following template to query research_paper_query_engine_tool tool: 'Provide title, sumary, authors and link to download for papers related to Quantum Computing'.'\n",
      "If there are not, could you fetch some papers from arXiv on this topic?\n",
      "\n",
      "IMPORTANT: do not download papers unless the user asks for it explicitly.\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: research_paper_query_engine_tool\n",
      "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Quantum Computing'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Title: None\n",
      "Summary: None\n",
      "Authors: None\n",
      "PDF url: None\n",
      "\u001b[0m> Running step 4da746be-0d85-4f0e-a7ee-2d730a7c006c. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I cannot answer the question with the provided tools.\n",
      "Answer: Unfortunately, I couldn't retrieve any papers related to Quantum Computing from the research paper database. Would you like me to fetch some papers from arXiv on this topic?\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "answer = agent.chat(query_template.format(topic=\"Quantum Computing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unfortunately, I couldn't retrieve any papers related to Quantum Computing from the research paper database. Would you like me to fetch some papers from arXiv on this topic?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
