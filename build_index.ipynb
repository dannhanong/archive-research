{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import fetch_arxiv_papers\n",
    "\n",
    "papers = fetch_arxiv_papers('language models', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Goldfish: Monolingual Language Models for 350 Languages',\n",
       " 'Counterfactually Probing Language Identity in Multilingual Models',\n",
       " 'Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification',\n",
       " 'Continuous multilinguality with language vectors',\n",
       " 'Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word Embedding Language Divergence (WELD) as a Quantitative Measure of Language Distance',\n",
       " 'The Geometry of Multilingual Language Model Representations',\n",
       " 'Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases',\n",
       " 'Cedille: A large autoregressive French language model',\n",
       " \"What's in a Name?\",\n",
       " 'Are All Languages Equally Hard to Language-Model?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paper['title'] for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "def create_documents_from_papers(papers):\n",
    "    documents = []\n",
    "    for paper in papers:\n",
    "        content = (\n",
    "            f'Title: {paper['title']}\\n'\n",
    "            f'Authors: {\", \".join(paper['authors'])}\\n'\n",
    "            f'Summary: {paper['summary']}\\n'\n",
    "            f'Journal: {paper['journal_ref']}\\n'\n",
    "            f'Doi: {paper['doi']}\\n'\n",
    "            f'Primary category: {paper['primary_category']}\\n'\n",
    "            f'Categories: {\", \".join(paper['categories'])}\\n'\n",
    "            f'PDF url: {paper['pdf_url']}\\n'\n",
    "        )\n",
    "        documents.append(Document(text = content))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents_from_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='09be6cbc-7f5b-40a2-bf06-02bbeaca497c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Goldfish: Monolingual Language Models for 350 Languages\\nAuthors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen\\nSummary: For many low-resource languages, the only available language models are large\\nmultilingual models trained on many languages simultaneously. However, using\\nFLORES perplexity as a metric, we find that these models perform worse than\\nbigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM\\n7.1B). To facilitate research that focuses on low-resource languages, we\\npre-train and release Goldfish, a suite of monolingual autoregressive\\nTransformer language models up to 125M parameters for 350 languages. The\\nGoldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98\\nof 204 FLORES languages, despite each Goldfish model being over 10x smaller.\\nHowever, the Goldfish significantly underperform larger multilingual models on\\nreasoning benchmarks, suggesting that for low-resource languages,\\nmultilinguality primarily improves general reasoning abilities rather than\\nbasic text generation. We release models trained on 5MB (350 languages), 10MB\\n(288 languages), 100MB (166 languages), and 1GB (83 languages) of text data\\nwhere available. The Goldfish models are available as baselines, fine-tuning\\nsources, or augmentations to existing models in low-resource NLP research, and\\nthey are further useful for crosslinguistic studies requiring maximally\\ncomparable models across languages.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nPDF url: http://arxiv.org/pdf/2408.10441v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3b04c31e-8523-420a-860b-34604220ede9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Counterfactually Probing Language Identity in Multilingual Models\\nAuthors: Anirudh Srinivasan, Venkata S Govindarajan, Kyle Mahowald\\nSummary: Techniques in causal analysis of language models illuminate how linguistic\\ninformation is organized in LLMs. We use one such technique, AlterRep, a method\\nof counterfactual probing, to explore the internal structure of multilingual\\nmodels (mBERT and XLM-R). We train a linear classifier on a binary language\\nidentity task, to classify tokens between Language X and Language Y. Applying a\\ncounterfactual probing procedure, we use the classifier weights to project the\\nembeddings into the null space and push the resulting embeddings either in the\\ndirection of Language X or Language Y. Then we evaluate on a masked language\\nmodeling task. We find that, given a template in Language X, pushing towards\\nLanguage Y systematically increases the probability of Language Y words, above\\nand beyond a third-party control language. But it does not specifically push\\nthe model towards translation-equivalent words in Language Y. Pushing towards\\nLanguage X (the same direction as the template) has a minimal effect, but\\nsomewhat degrades these models. Overall, we take these results as further\\nevidence of the rich structure of massive multilingual language models, which\\ninclude both a language-specific and language-general component. And we show\\nthat counterfactual probing can be fruitfully applied to multilingual models.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nPDF url: http://arxiv.org/pdf/2310.18862v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d9148cca-373f-4979-a883-2ef42e1ea063', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification\\nAuthors: Luis Quesada, Fernando Berzal, Francisco J. Cortijo\\nSummary: Model-based language specification has applications in the implementation of\\nlanguage processors, the design of domain-specific languages, model-driven\\nsoftware development, data integration, text mining, natural language\\nprocessing, and corpus-based induction of models. Model-based language\\nspecification decouples language design from language processing and, unlike\\ntraditional grammar-driven approaches, which constrain language designers to\\nspecific kinds of grammars, it needs general parser generators able to deal\\nwith ambiguities. In this paper, we propose Fence, an efficient bottom-up\\nparsing algorithm with lexical and syntactic ambiguity support that enables the\\nuse of model-based language specification in practice.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nPDF url: http://arxiv.org/pdf/1107.4687v2\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='45f74c44-5b70-4f4e-852b-e6093bf7bf0e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Continuous multilinguality with language vectors\\nAuthors: Robert Östling, Jörg Tiedemann\\nSummary: Most existing models for multilingual natural language processing (NLP) treat\\nlanguage as a discrete category, and make predictions for either one language\\nor the other. In contrast, we propose using continuous vector representations\\nof language. We show that these can be learned efficiently with a\\ncharacter-based neural language model, and used to improve inference about\\nlanguage varieties not seen during training. In experiments with 1303 Bible\\ntranslations into 990 different languages, we empirically explore the capacity\\nof multilingual language models, and also show that the language vectors\\ncapture genetic relationships between languages.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nPDF url: http://arxiv.org/pdf/1612.07486v2\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='64b58e56-910d-4bbc-8508-3b8430e416f3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word Embedding Language Divergence (WELD) as a Quantitative Measure of Language Distance\\nAuthors: Ehsaneddin Asgari, Mohammad R. K. Mofrad\\nSummary: We introduce a new measure of distance between languages based on word\\nembedding, called word embedding language divergence (WELD). WELD is defined as\\ndivergence between unified similarity distribution of words between languages.\\nUsing such a measure, we perform language comparison for fifty natural\\nlanguages and twelve genetic languages. Our natural language dataset is a\\ncollection of sentence-aligned parallel corpora from bible translations for\\nfifty languages spanning a variety of language families. Although we use\\nparallel corpora, which guarantees having the same content in all languages,\\ninterestingly in many cases languages within the same family cluster together.\\nIn addition to natural languages, we perform language comparison for the coding\\nregions in the genomes of 12 different organisms (4 plants, 6 animals, and two\\nhuman subjects). Our result confirms a significant high-level difference in the\\ngenetic language model of humans/animals versus plants. The proposed method is\\na step toward defining a quantitative measure of similarity between languages,\\nwith applications in languages classification, genre identification, dialect\\nidentification, and evaluation of translations.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nPDF url: http://arxiv.org/pdf/1604.08561v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e02c14c1-4564-436a-b7c2-0fd87db5a7ac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: The Geometry of Multilingual Language Model Representations\\nAuthors: Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen\\nSummary: We assess how multilingual language models maintain a shared multilingual\\nrepresentation space while still encoding language-sensitive information in\\neach language. Using XLM-R as a case study, we show that languages occupy\\nsimilar linear subspaces after mean-centering, evaluated based on causal\\neffects on language modeling performance and direct comparisons between\\nsubspaces for 88 languages. The subspace means differ along language-sensitive\\naxes that are relatively stable throughout middle layers, and these axes encode\\ninformation such as token vocabularies. Shifting representations by language\\nmeans is sufficient to induce token predictions in different languages.\\nHowever, we also identify stable language-neutral axes that encode information\\nsuch as token positions and part-of-speech. We visualize representations\\nprojected onto language-sensitive and language-neutral axes, identifying\\nlanguage family and part-of-speech clusters, along with spirals, toruses, and\\ncurves representing token position information. These results demonstrate that\\nmultilingual language models encode information along orthogonal\\nlanguage-sensitive and language-neutral axes, allowing the models to extract a\\nvariety of features for downstream tasks and cross-lingual transfer learning.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nPDF url: http://arxiv.org/pdf/2205.10964v2\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='44f66d1c-91a1-4cfb-a26c-3f8350dc4042', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases\\nAuthors: Michael Y. Hu, Jackson Petty, Chuan Shi, William Merrill, Tal Linzen\\nSummary: Pretraining language models on formal languages can improve their acquisition\\nof natural language, but it is unclear which features of the formal language\\nimpart an inductive bias that leads to effective transfer. Drawing on insights\\nfrom linguistics and complexity theory, we hypothesize that effective transfer\\noccurs when the formal language both captures dependency structures in natural\\nlanguage and remains within the computational limitations of the model\\narchitecture. Focusing on transformers, we find that formal languages with both\\nthese properties enable language models to achieve lower loss on natural\\nlanguage and better linguistic generalization compared to other languages. In\\nfact, pre-pretraining, or training on formal-then-natural language, reduces\\nloss more efficiently than the same amount of natural language. For a\\n1B-parameter language model trained on roughly 1.6B tokens of natural language,\\npre-pretraining achieves the same loss and better linguistic generalization\\nwith a 33% smaller token budget. We also give mechanistic evidence of\\ncross-task transfer from formal to natural language: attention heads acquired\\nduring formal language pretraining remain crucial for the model's performance\\non syntactic evaluations.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL, cs.AI, cs.LG\\nPDF url: http://arxiv.org/pdf/2502.19249v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='111a2cee-524f-43f4-b367-9fe64c9568a6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Cedille: A large autoregressive French language model\\nAuthors: Martin Müller, Florian Laurent\\nSummary: Scaling up the size and training of autoregressive language models has\\nenabled novel ways of solving Natural Language Processing tasks using zero-shot\\nand few-shot learning. While extreme-scale language models such as GPT-3 offer\\nmultilingual capabilities, zero-shot learning for languages other than English\\nremain largely unexplored. Here, we introduce Cedille, a large open source\\nauto-regressive language model, specifically trained for the French language.\\nOur results show that Cedille outperforms existing French language models and\\nis competitive with GPT-3 on a range of French zero-shot benchmarks.\\nFurthermore, we provide an in-depth comparison of the toxicity exhibited by\\nthese models, showing that Cedille marks an improvement in language model\\nsafety thanks to dataset filtering.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL, 68T50, I.2.7\\nPDF url: http://arxiv.org/pdf/2202.03371v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4aa977a1-3cbb-4c73-b6fd-82b03c4dfcfc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: What's in a Name?\\nAuthors: Stasinos Konstantopoulos\\nSummary: This paper describes experiments on identifying the language of a single name\\nin isolation or in a document written in a different language. A new corpus has\\nbeen compiled and made available, matching names against languages. This corpus\\nis used in a series of experiments measuring the performance of general\\nlanguage models and names-only language models on the language identification\\ntask. Conclusions are drawn from the comparison between using general language\\nmodels and names-only language models and between identifying the language of\\nisolated names and the language of very short document fragments. Future\\nresearch directions are outlined.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL, cs.AI\\nPDF url: http://arxiv.org/pdf/0710.1481v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1898717d-8a8c-4c70-85ce-dd87800e2ea7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Are All Languages Equally Hard to Language-Model?\\nAuthors: Ryan Cotterell, Sabrina J. Mielke, Jason Eisner, Brian Roark\\nSummary: For general modeling methods applied to diverse languages, a natural question\\nis: how well should we expect our models to work on languages with differing\\ntypological profiles? In this work, we develop an evaluation framework for fair\\ncross-linguistic comparison of language models, using translated text so that\\nall models are asked to predict approximately the same information. We then\\nconduct a study on 21 languages, demonstrating that in some languages, the\\ntextual expression of the information is harder to predict with both $n$-gram\\nand LSTM language models. We show complex inflectional morphology to be a cause\\nof performance differences among languages.\\nJournal: None\\nDoi: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nPDF url: http://arxiv.org/pdf/1806.03743v2\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from constants import embed_model\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\"index/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
